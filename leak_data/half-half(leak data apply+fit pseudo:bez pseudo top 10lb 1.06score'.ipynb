{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half and Half\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading train data\n",
    "Reading train data along with building and weather metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "building = pd.read_csv('data/building_metadata_external.csv')\n",
    "le = LabelEncoder()\n",
    "building.primary_use = le.fit_transform(building.primary_use)\n",
    "\n",
    "weather_train = pd.read_csv('data/weather_train.csv')\n",
    "weather_test = pd.read_csv('data/weather_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.09 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 76.4%\n",
      "Memory usage of dataframe is 8.58 MB\n",
      "Memory usage after optimization is: 2.26 MB\n",
      "Decreased by 73.7%\n",
      "Memory usage of dataframe is 17.11 MB\n",
      "Memory usage after optimization is: 4.50 MB\n",
      "Decreased by 73.7%\n"
     ]
    }
   ],
   "source": [
    "from tools import reduce_mem_usage\n",
    "\n",
    "# df_train = reduce_mem_usage(df_train, use_float16=True)\n",
    "building = reduce_mem_usage(building, use_float16=True)\n",
    "weather_train = reduce_mem_usage(weather_train, use_float16=True)\n",
    "weather_test = reduce_mem_usage(weather_test, use_float16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "in_us = [0,2,3,4,6,8,9,10,13,14,15]\n",
    "in_ca = [7,11]\n",
    "in_uk = [1,5]\n",
    "in_ie = [12]\n",
    "\n",
    "us_cal =  holidays.US()\n",
    "ca_cal = holidays.CA()\n",
    "ie_cal = holidays.IE()\n",
    "uk_cal = holidays.UK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_name(timestamp, site_id):\n",
    "    if site_id in in_ca:\n",
    "        return ca_cal.get(timestamp)\n",
    "    elif site_id in in_uk:\n",
    "        return uk_cal.get(timestamp)\n",
    "    elif site_id in in_ie:\n",
    "        return ie_cal.get(timestamp)\n",
    "    else:\n",
    "        return us_cal.get(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_holidays(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['holiday_name'] = df.apply(lambda x: holiday_name(x.timestamp, x.site_id), axis=1)\n",
    "    df['holiday_name'] = df['holiday_name'].astype('category')\n",
    "    df['holiday_name'] = df['holiday_name'].cat.add_categories(['NONE'])\n",
    "    df['holiday_name'] = df['holiday_name'].fillna('NONE')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relative_humidity(df):\n",
    "#     exp = pd.np.exp\n",
    "#     df['relative_humidity'] = 100*(exp((17.625*df['dew_temperature'])/(243.04+df['dew_temperature'])) / exp((17.625*df['air_temperature'])/(243.04+df['air_temperature'])))\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_lag_feature(weather_df, window=3):\n",
    "#     group_df = weather_df.groupby('site_id')\n",
    "#     cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'wind_direction', 'wind_speed']\n",
    "#     rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "#     lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "#     lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "#     lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "#     lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "#     for col in cols:\n",
    "#         weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "#         weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "#         weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "#         weather_df[f'{col}_std_lag{window}'] = lag_std[col]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = transform_holidays(weather_train)\n",
    "# le_weather = LabelEncoder()\n",
    "# weather_train.holiday_name = le_weather.fit_transform(weather_train.holiday_name)\n",
    "\n",
    "weather_test = transform_holidays(weather_test)\n",
    "# weather_test.holiday_name = le_weather.transform(weather_test.holiday_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_data(df, scale_list):\n",
    "#     mean = df[scale_list].mean(axis=0)\n",
    "#     df[scale_list] = df[scale_list].astype('float32')\n",
    "#     df[scale_list] -= df[scale_list].mean(axis=0)\n",
    "#     std = df[scale_list].std(axis=0)\n",
    "#     df[scale_list] /= df[scale_list].std(axis=0)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_data(self, X, building_data, weather_data, test=False):\n",
    "\n",
    "        X.timestamp = pd.to_datetime(X.timestamp)\n",
    "        X.timestamp = X.timestamp.astype('datetime64[ns]')\n",
    "\n",
    "        X = X.merge(building_data, on=\"building_id\", how=\"left\")\n",
    "        X = X.merge(weather_data, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
    "\n",
    "        if not test:\n",
    "            df_group = X.groupby('building_id')['meter_reading']\n",
    "            self.building_mean = df_group.mean().astype(np.float16)\n",
    "            self.building_median = df_group.median().astype(np.float16)\n",
    "            self.building_min = df_group.min().astype(np.float16)\n",
    "            self.building_max = df_group.max().astype(np.float16)\n",
    "            self.building_std = df_group.std().astype(np.float16)\n",
    "\n",
    "        X['building_mean'] = X['building_id'].map(self.building_mean)\n",
    "        X['building_median'] = X['building_id'].map(self.building_median)\n",
    "        X['building_min'] = X['building_id'].map(self.building_min)\n",
    "        X['building_max'] = X['building_id'].map(self.building_max)\n",
    "        X['building_std'] = X['building_id'].map(self.building_std)\n",
    "\n",
    "        X.sort_values(\"timestamp\")\n",
    "        X.reset_index(drop=True)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "        X.timestamp = pd.to_datetime(X.timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        X.square_feet = np.log1p(X.square_feet)\n",
    "\n",
    "        X[\"hour\"] = X.timestamp.dt.hour\n",
    "        X[\"weekday\"] = X.timestamp.dt.weekday\n",
    "    #     X['month'] = X.timestamp.dt.month\n",
    "    #     X['day'] = X.timestamp.dt.day\n",
    "    #     X = encode_date(X)\n",
    "\n",
    "        X['is_holiday'] = X['holiday_name'].apply(lambda x: 1 if x != \"NONE\" else 0)\n",
    "\n",
    "\n",
    "\n",
    "        if not test:\n",
    "            X.drop(index=X[(X.meter_reading <=0) &\n",
    "                           (X.meter == 0)].index, inplace=True)\n",
    "\n",
    "    #     drop_features = [\"timestamp\", \"wind_direction\", \"wind_speed\", 'holiday_name']\n",
    "        drop_features = [\"wind_direction\", \"wind_speed\", 'holiday_name']\n",
    "\n",
    "\n",
    "\n",
    "        X.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "        if test:\n",
    "            row_ids = X.row_id\n",
    "            X.drop(\"row_id\", axis=1, inplace=True)\n",
    "            return X, row_ids\n",
    "        else:\n",
    "            y = np.log1p(X.meter_reading)\n",
    "            X.drop([\"meter_reading\",'timestamp'], axis=1, inplace=True)\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepr = DataPrep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = prepr.prepare_data(df_train, building, weather_train, False)\n",
    "del df_train, weather_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('pseudo_data/X_pseudo.pkl')\n",
    "y_train = pd.read_pickle('pseudo_data/y_pseudo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # weather_train = timestamp_align(weather_train)\n",
    "# X_train, y_train = prepare_data(df_train, building, weather_train)\n",
    "\n",
    "# # del df_train, weather_train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-fold LightGBM Model split half-and-half\n",
    "The data is split into two based on time. Each half is used as the training data for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with first half and validating on second half:\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[300]\ttraining's rmse: 0.785985\tvalid_1's rmse: 1.05302\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-348f89326f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model with first half and validating on second half:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel_half_1_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_half_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatchlist_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model with second half and validating on first half:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_half_1 = X_train[:int(X_train.shape[0] / 2)]\n",
    "X_half_2 = X_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "y_half_1 = y_train[:int(X_train.shape[0] / 2)]\n",
    "y_half_2 = y_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "\n",
    "\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\",\n",
    "                        \"hour\", \"weekday\", 'is_holiday']\n",
    "\n",
    "\n",
    "\n",
    "d_half_1 = lgb.Dataset(X_half_1, label=y_half_1, categorical_feature=categorical_features, free_raw_data=False)\n",
    "d_half_2 = lgb.Dataset(X_half_2, label=y_half_2, categorical_feature=categorical_features, free_raw_data=False)\n",
    "\n",
    "watchlist_1 = [d_half_1, d_half_2]\n",
    "watchlist_2 = [d_half_2, d_half_1]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 50,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"metric\": \"rmse\",\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Building model with first half and validating on second half:\")\n",
    "model_half_1_a = lgb.train(params, train_set=d_half_1, num_boost_round=1500, valid_sets=watchlist_1, verbose_eval=300, early_stopping_rounds=300)\n",
    "\n",
    "print(\"Building model with second half and validating on first half:\")\n",
    "model_half_2_a = lgb.train(params, train_set=d_half_2, num_boost_round=3000, valid_sets=watchlist_2, verbose_eval=300, early_stopping_rounds=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_half_1, X_half_2, y_half_1, y_half_2, d_half_1, d_half_2, watchlist_1, watchlist_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data\n",
    "Preparing test data with same features as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1272.51 MB\n",
      "Memory usage after optimization is: 358.65 MB\n",
      "Decreased by 71.8%\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/test.csv')\n",
    "leak = pd.read_pickle('data/site0.pkl')\n",
    "\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "X_test, row_ids = prepr.prepare_data(df_test, building, weather_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smart_test(df_test):\n",
    "#     model_part_1 = df_test[df_test.timestamp < pd.to_datetime('2017-07-07 22:00:00')]\n",
    "\n",
    "#     second_part = df_test[(df_test.timestamp >= pd.to_datetime('2017-07-07 22:00:00')) & (df_test.timestamp <= pd.to_datetime('2017-12-31 23:00:00'))]\n",
    "\n",
    "#     model_part_1a = df_test[df_test.timestamp >= pd.to_datetime('2018-01-01 00:00:00')]\n",
    "    \n",
    "#     return model_part_1.drop('timestamp', axis=1), second_part.drop('timestamp', axis=1),\\\n",
    "#            model_part_1a.drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_1, b_2, c_1 = smart_test(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# del df_test, building, weather_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_a_1 = np.expm1(model_half_1.predict(a_1, num_iteration=model_half_1.best_iteration))\n",
    "# pred_b_2 = np.expm1(model_half_2.predict(b_2, num_iteration=model_half_2.best_iteration))\n",
    "# pred_c_1 = np.expm1(model_half_1.predict(c_1, num_iteration=model_half_1.best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_pred = np.concatenate([pred_a_1, pred_b_2, pred_c_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring test data\n",
    "Averaging predictions from the two half train data models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.expm1(model_half_1.predict(X_test.drop('timestamp', axis=1), num_iteration=model_half_1.best_iteration)) / 2\n",
    "\n",
    "del model_half_1\n",
    "gc.collect()\n",
    "\n",
    "pred += np.expm1(model_half_2.predict(X_test.drop('timestamp', axis=1), num_iteration=model_half_2.best_iteration)) / 2\n",
    "    \n",
    "del model_half_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_leaks(leak, df_test):\n",
    "    X = df_test.merge(leak, on=[\"building_id\",'timestamp','meter'], how=\"left\")\n",
    "    X.drop('meter_reading_original', axis=1, inplace=True)\n",
    "    leak_target = X['meter_reading_scraped']\n",
    "    \n",
    "    return leak_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked = apply_leaks(leak, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leak_df = pd.DataFrame(leaked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df['pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meter_reading_scraped</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.370293</td>\n",
       "      <td>154.514187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.512720</td>\n",
       "      <td>75.474717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.143042</td>\n",
       "      <td>9.710868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.701470</td>\n",
       "      <td>211.103380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1141.240666</td>\n",
       "      <td>952.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.149443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.436384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.565448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>NaN</td>\n",
       "      <td>177.651064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.387310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          meter_reading_scraped        pred\n",
       "0                    173.370293  154.514187\n",
       "1                     53.512720   75.474717\n",
       "2                      6.143042    9.710868\n",
       "3                    101.701470  211.103380\n",
       "4                   1141.240666  952.718400\n",
       "...                         ...         ...\n",
       "41697595                    NaN    7.149443\n",
       "41697596                    NaN    4.436384\n",
       "41697597                    NaN    4.565448\n",
       "41697598                    NaN  177.651064\n",
       "41697599                    NaN    3.387310\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df.meter_reading_scraped.fillna(leak_df.pred, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_pred = leak_df['meter_reading_scraped'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173.37029336,  53.51271968,   6.14304201, ...,   4.56544832,\n",
       "       177.65106441,   3.38731016])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaked_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame((pred + total_pred) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Preparing final file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(leaked_pred, 0, a_max=None)})\n",
    "submission.to_csv(\"submission_leak2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
